"""
rl_train_and_eval.py

Train an offline RL agent (CQL) using d3rlpy on the prepared RL dataset.

Also includes a simple one-step Importance Sampling OPE estimator (since episodes are 1-step).

Usage:
    python src/rl_train_and_eval.py --dataset ../data/processed/rl_dataset.npz --out models/rl/
"""

import os
import argparse
import numpy as np
import joblib
from d3rlpy.algos import CQL, IQL
from d3rlpy.dataset import MDPDataset
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

SEED = 42
np.random.seed(SEED)

def load_npz_dataset(path):
    data = np.load(path)
    obs = data['obs']
    acts = data['acts']
    rewards = data['rewards']
    next_obs = data['next_obs']
    terminals = data['terminals']
    return obs, acts, rewards, next_obs, terminals

def make_d3rlpy_dataset(obs, acts, rewards, next_obs, terminals):
    # d3rlpy expects lists per episode; we will create one-step episodes
    observations = [obs]  # shape (n_steps, obs_dim) but for single episode; simpler: use MDPDataset.build
    dataset = MDPDataset(observations=obs, actions=acts, rewards=rewards, terminals=terminals)
    return dataset

def estimate_behavior_policy(obs, acts):
    """Fit a simple classifier pi_b(a|s) (here binary)"""
    clf = LogisticRegression(max_iter=1000)
    clf.fit(obs, acts)
    return clf

def importance_sampling_estimate(eval_policy, behavior_clf, obs, acts, rewards):
    """
    Since episodes are one-step:
      IS estimate = mean_i [ (pi_e(a_i | s_i) / pi_b(a_i | s_i)) * r_i ]
    eval_policy should expose predict_proba(obs) -> prob of action 1
    """
    # behavior probs
    pb = behavior_clf.predict_proba(obs)  # shape (n, 2)
    pe = eval_policy.predict_proba(obs)   # assume same format
    # prob of actually taken action under each policy
    taken = acts.astype(int)
    pb_a = np.array([pb[i, taken[i]] for i in range(len(acts))])
    pe_a = np.array([pe[i, taken[i]] for i in range(len(acts))])
    # avoid div by zero
    eps = 1e-8
    weights = pe_a / (pb_a + eps)
    estimate = np.mean(weights * rewards)
    return float(estimate), float(np.std(weights * rewards) / np.sqrt(len(rewards)))

class EvalPolicyWrapper:
    """Wrap a d3rlpy policy to produce action probabilities for a/b test."""
    def __init__(self, policy, n_actions=2):
        self.policy = policy
        self.n_actions = n_actions
    def predict_proba(self, obs):
        # obs: (n, obs_dim)
        # we will get deterministic action probabilities by calling policy. For stochastic, sample.
        # For simplicity, we get Q-values and convert via softmax (not ideal for real OPE)
        qs = self.policy.predict_value(obs)  # returns q-values shape (n, n_actions)
        # softmax
        exp_q = np.exp(qs - qs.max(axis=1, keepdims=True))
        probs = exp_q / exp_q.sum(axis=1, keepdims=True)
        return probs

def main(args):
    os.makedirs(args.out, exist_ok=True)
    obs, acts, rewards, next_obs, terminals = load_npz_dataset(args.dataset)
    print('Loaded dataset shapes:', obs.shape, acts.shape, rewards.shape)

    # Build d3rlpy dataset
    dataset = MDPDataset(observations=obs, actions=acts, rewards=rewards, terminals=terminals)

    # Choose algorithm (CQL)
    # Instantiate algorithm in a robust, version-agnostic way.
    # Different d3rlpy versions require different constructor signatures.
    # We'll inspect the constructor and try a few safe options.
    import inspect
    def make_algo(cls):
        sig = inspect.signature(cls)
        params = list(sig.parameters.keys())
        # Try no-arg construction first
        try:
            return cls()
        except Exception:
            pass
        # Try (config, device, enable_ddp) style (positional)
        try:
            return cls(None, device='cpu', enable_ddp=False)
        except Exception:
            pass
        # Try keyword args if parameter names exist
        kwargs = {}
        if 'config' in params:
            kwargs['config'] = None
        if 'device' in params:
            kwargs['device'] = 'cpu'
        if 'enable_ddp' in params:
            kwargs['enable_ddp'] = False
        try:
            return cls(**kwargs)
        except Exception:
            pass
        # As a last attempt, call with a single dummy config object if supported
        try:
            return cls({'dummy': True})
        except Exception as e:
            # If we still fail, raise the original error so the user can see details
            raise RuntimeError(f'Could not construct {cls} with tried signatures; last error: {e}')
    if args.algo.lower() == 'cql':
        algo = make_algo(CQL)
    else:
        algo = make_algo(IQL)

    # Fit (small number of steps as example)
    print('Training offline RL algorithm...')
    algo.fit(dataset, n_steps=args.steps)

    policy_path = os.path.join(args.out, f'{args.algo}_policy.zip')
    algo.save_policy(policy_path)
    print('Saved policy to', policy_path)

    # For OPE: estimate behavior policy
    behavior_clf = estimate_behavior_policy(obs, acts)
    # Export behavior_clf
    joblib.dump(behavior_clf, os.path.join(args.out, 'behavior_clf.joblib'))

    # Wrap learned policy
    learned_policy = algo.create_policy()  # d3rlpy policy wrapper
    eval_wrapper = EvalPolicyWrapper(learned_policy)

    # Estimate IS
    est, stderr = importance_sampling_estimate(eval_wrapper, behavior_clf, obs, acts, rewards)
    print(f'IS estimated policy value: {est:.4f} +/- {1.96*stderr:.4f} (95% CI approx)')

    # Save results
    with open(os.path.join(args.out, 'rl_metrics.txt'), 'w') as f:
        f.write(f'IS_estimate: {est}\nstderr:{stderr}\n')

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default='../data/processed/rl_dataset.npz')
    parser.add_argument('--out', type=str, default='models/rl/')
    parser.add_argument('--algo', type=str, default='cql')
    parser.add_argument('--steps', type=int, default=20000)
    args = parser.parse_args()
    main(args)